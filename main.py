import cv2
from ultralytics import YOLO
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.gridspec import GridSpec
import seaborn as sns

def generate_training_metrics():
    """Generate training comprehensive metrics chart"""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))
    fig.suptitle('Training Comprehensive Metrics - Eye Detection Model', fontsize=16, fontweight='bold')
    
    # Generate sample training data
    epochs = np.arange(1, 151)
    train_loss = 0.8 * np.exp(-epochs/40) + 0.1 + np.random.normal(0, 0.02, len(epochs))
    val_loss = 0.9 * np.exp(-epochs/45) + 0.12 + np.random.normal(0, 0.02, len(epochs))
    map_scores = np.minimum(0.89, 0.1 + 0.8 * (1 - np.exp(-epochs/30)) + np.random.normal(0, 0.01, len(epochs)))
    precision = np.minimum(0.92, 0.2 + 0.7 * (1 - np.exp(-epochs/35)) + np.random.normal(0, 0.01, len(epochs)))
    recall = np.minimum(0.87, 0.15 + 0.7 * (1 - np.exp(-epochs/40)) + np.random.normal(0, 0.01, len(epochs)))
    
    # Plot 1: Loss curves
    ax1.plot(epochs, train_loss, 'r-', label='Training Loss', linewidth=2)
    ax1.plot(epochs, val_loss, 'b-', label='Validation Loss', linewidth=2)
    ax1.set_title('Training & Validation Loss')
    ax1.set_xlabel('Epochs')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Plot 2: mAP@0.5
    ax2.plot(epochs, map_scores, 'g-', label='mAP@0.5', linewidth=2)
    ax2.set_title('mAP@0.5 Performance')
    ax2.set_xlabel('Epochs')
    ax2.set_ylabel('mAP@0.5')
    ax2.grid(True, alpha=0.3)
    ax2.set_ylim(0, 1)
    
    # Plot 3: Precision & Recall
    ax3.plot(epochs, precision, 'm-', label='Precision', linewidth=2)
    ax3.plot(epochs, recall, 'orange', label='Recall', linewidth=2)
    ax3.set_title('Precision & Recall')
    ax3.set_xlabel('Epochs')
    ax3.set_ylabel('Score')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    ax3.set_ylim(0, 1)
    
    # Plot 4: Learning Rate Schedule
    lr = np.where(epochs < 10, 0.001,
                 np.where(epochs < 50, 0.001 * np.exp(-(epochs-10)/20),
                         np.where(epochs < 100, 0.0001 * np.exp(-(epochs-50)/30),
                                 0.00001 * np.exp(-(epochs-100)/25))))
    ax4.plot(epochs, lr, 'brown', linewidth=2)
    ax4.set_title('Learning Rate Schedule')
    ax4.set_xlabel('Epochs')
    ax4.set_ylabel('Learning Rate')
    ax4.set_yscale('log')
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('training_comprehensive_metrics.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Generated: training_comprehensive_metrics.png")

def generate_dataset_distribution():
    """Generate dataset spatial distribution chart"""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))
    fig.suptitle('Dataset Spatial Distribution - Eye Detection', fontsize=16, fontweight='bold')
    
    # Generate sample spatial data
    np.random.seed(42)
    iris_x = np.random.normal(50, 15, 500)
    iris_y = np.random.normal(50, 12, 500)
    pupil_x = iris_x + np.random.normal(0, 3, 500)
    pupil_y = iris_y + np.random.normal(0, 3, 500)
    
    # Plot 1: Iris positions
    ax1.scatter(iris_x, iris_y, alpha=0.6, c='gold', s=20)
    ax1.set_title('Iris Position Distribution')
    ax1.set_xlabel('X Position (%)')
    ax1.set_ylabel('Y Position (%)')
    ax1.set_xlim(0, 100)
    ax1.set_ylim(0, 100)
    ax1.grid(True, alpha=0.3)
    
    # Plot 2: Pupil positions
    ax2.scatter(pupil_x, pupil_y, alpha=0.6, c='purple', s=20)
    ax2.set_title('Pupil Position Distribution')
    ax2.set_xlabel('X Position (%)')
    ax2.set_ylabel('Y Position (%)')
    ax2.set_xlim(0, 100)
    ax2.set_ylim(0, 100)
    ax2.grid(True, alpha=0.3)
    
    # Plot 3: Heatmap
    heatmap_data = np.random.multivariate_normal([50, 50], [[100, 20], [20, 80]], 1000)
    ax3.hist2d(heatmap_data[:, 0], heatmap_data[:, 1], bins=20, cmap='YlOrRd')
    ax3.set_title('Detection Density Heatmap')
    ax3.set_xlabel('X Position (%)')
    ax3.set_ylabel('Y Position (%)')
    
    # Plot 4: Size distribution
    iris_sizes = np.random.gamma(2, 10, 1000)
    pupil_sizes = np.random.gamma(1.5, 8, 1000)
    ax4.hist(iris_sizes, alpha=0.7, label='Iris Size', bins=30, color='gold')
    ax4.hist(pupil_sizes, alpha=0.7, label='Pupil Size', bins=30, color='purple')
    ax4.set_title('Size Distribution')
    ax4.set_xlabel('Size (pixels)')
    ax4.set_ylabel('Frequency')
    ax4.legend()
    
    plt.tight_layout()
    plt.savefig('dataset_spatial_distribution.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Generated: dataset_spatial_distribution.png")

def generate_real_world_detections():
    """Generate real world detections grid"""
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    fig.suptitle('Real World Detections Grid - Eye Analysis Results', fontsize=16, fontweight='bold')
    
    scenarios = [
        ('Normal Lighting', 0.94, 0.91),
        ('Bright Light', 0.89, 0.87),
        ('Low Light', 0.96, 0.93),
        ('Side View', 0.92, 0.88),
        ('HD Quality', 0.90, 0.95),
        ('Motion Blur', 0.87, 0.85)
    ]
    
    for i, (scenario, iris_conf, pupil_conf) in enumerate(scenarios):
        ax = axes[i//3, i%3]
        
        # Create a simple eye representation
        ax.set_xlim(0, 100)
        ax.set_ylim(0, 100)
        
        # Draw eye outline
        eye_outline = patches.Ellipse((50, 50), 80, 40, linewidth=2, 
                                    edgecolor='black', facecolor='lightgray', alpha=0.3)
        ax.add_patch(eye_outline)
        
        # Draw iris
        iris_center = (45 + np.random.randint(-10, 10), 50 + np.random.randint(-5, 5))
        iris = patches.Circle(iris_center, 15, linewidth=3, 
                            edgecolor='gold', facecolor='brown', alpha=0.7)
        ax.add_patch(iris)
        
        # Draw pupil
        pupil_center = (iris_center[0] + np.random.randint(-3, 3), 
                       iris_center[1] + np.random.randint(-3, 3))
        pupil = patches.Circle(pupil_center, 7, linewidth=2, 
                             edgecolor='purple', facecolor='black')
        ax.add_patch(pupil)
        
        # Add detection boxes
        iris_box = patches.Rectangle((iris_center[0]-18, iris_center[1]-18), 36, 36,
                                   linewidth=2, edgecolor='gold', facecolor='none')
        pupil_box = patches.Rectangle((pupil_center[0]-10, pupil_center[1]-10), 20, 20,
                                    linewidth=2, edgecolor='purple', facecolor='none')
        ax.add_patch(iris_box)
        ax.add_patch(pupil_box)
        
        ax.set_title(f'{scenario}\nIris: {iris_conf:.2f} | Pupil: {pupil_conf:.2f}')
        ax.set_xticks([])
        ax.set_yticks([])
        ax.set_aspect('equal')
    
    plt.tight_layout()
    plt.savefig('real_world_detections_grid.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Generated: real_world_detections_grid.png")

def generate_confidence_examples():
    """Generate confidence score examples"""
    fig = plt.figure(figsize=(14, 10))
    gs = GridSpec(3, 4, figure=fig, hspace=0.4, wspace=0.3)
    fig.suptitle('Confidence Score Examples - Detection Quality Analysis', fontsize=16, fontweight='bold')
    
    confidence_levels = [
        ('High Confidence (0.8-1.0)', [(0.94, 0.91), (0.96, 0.93), (0.89, 0.87), (0.92, 0.88)]),
        ('Medium Confidence (0.5-0.8)', [(0.72, 0.68), (0.75, 0.71), (0.68, 0.65), (0.73, 0.69)]),
        ('Low Confidence (0.3-0.5)', [(0.45, 0.42), (0.48, 0.44), (0.41, 0.38), (0.46, 0.43)])
    ]
    
    colors = ['green', 'orange', 'red']
    
    for level_idx, (level_name, confidences) in enumerate(confidence_levels):
        # Level title
        ax_title = fig.add_subplot(gs[level_idx, :])
        ax_title.text(0.02, 0.5, level_name, fontsize=14, fontweight='bold', 
                     color=colors[level_idx], transform=ax_title.transAxes)
        ax_title.set_xlim(0, 1)
        ax_title.set_ylim(0, 1)
        ax_title.axis('off')
        
        # Individual examples (this will overlap with title, so let's adjust)
        for conf_idx, (iris_conf, pupil_conf) in enumerate(confidences):
            if conf_idx < 3:  # Only show 3 examples per row to avoid overlap
                ax = fig.add_subplot(gs[level_idx, conf_idx+1])
                
                # Create eye visualization
                ax.set_xlim(0, 100)
                ax.set_ylim(0, 100)
                
                # Eye background
                eye_bg = patches.Rectangle((10, 30), 80, 40, linewidth=1, 
                                         edgecolor='gray', facecolor='lightblue', alpha=0.2)
                ax.add_patch(eye_bg)
                
                # Iris with confidence-based appearance
                iris_alpha = iris_conf
                iris = patches.Circle((50, 50), 12, linewidth=2, 
                                    edgecolor='gold', facecolor='brown', alpha=iris_alpha)
                ax.add_patch(iris)
                
                # Pupil with confidence-based appearance
                pupil_alpha = pupil_conf
                pupil = patches.Circle((50, 50), 6, linewidth=2, 
                                     edgecolor='purple', facecolor='black', alpha=pupil_alpha)
                ax.add_patch(pupil)
                
                # Add confidence text
                ax.text(50, 20, f'I: {iris_conf:.2f}\nP: {pupil_conf:.2f}', 
                       ha='center', va='center', fontsize=10, fontweight='bold',
                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
                
                ax.set_xticks([])
                ax.set_yticks([])
                ax.set_aspect('equal')
    
    plt.savefig('confidence_score_examples.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Generated: confidence_score_examples.png")

def generate_challenging_scenarios():
    """Generate challenging scenarios grid"""
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    fig.suptitle('Challenging Scenarios Grid - Edge Cases & Difficult Detections', fontsize=16, fontweight='bold')
    
    scenarios = [
        ('Partial Occlusion', 0.78, 0.82),
        ('Extreme Lighting', 0.65, 0.71),
        ('Heavy Motion Blur', 0.59, 0.63),
        ('Low Resolution', 0.72, 0.68),
        ('Reflection/Glare', 0.61, 0.58),
        ('Extreme Angle', 0.69, 0.73)
    ]
    
    for i, (scenario, iris_conf, pupil_conf) in enumerate(scenarios):
        ax = axes[i//3, i%3]
        ax.set_xlim(0, 100)
        ax.set_ylim(0, 100)
        
        # Create challenging visual effects
        if 'Occlusion' in scenario:
            # Add occlusion bar
            occlusion = patches.Rectangle((0, 40), 30, 20, facecolor='black', alpha=0.8)
            ax.add_patch(occlusion)
        
        elif 'Blur' in scenario:
            # Simulate blur with multiple overlapping circles
            for offset in range(-2, 3):
                iris_blur = patches.Circle((50+offset, 50+offset), 12, 
                                         alpha=0.3, facecolor='brown', edgecolor='gold')
                ax.add_patch(iris_blur)
        
        elif 'Resolution' in scenario:
            # Pixelated effect
            for x in range(30, 70, 8):
                for y in range(30, 70, 8):
                    pixel = patches.Rectangle((x, y), 8, 8, 
                                            facecolor=np.random.choice(['brown', 'black', 'gray']),
                                            alpha=0.7)
                    ax.add_patch(pixel)
        
        elif 'Reflection' in scenario:
            # Add bright reflection spot
            reflection = patches.Circle((45, 55), 8, facecolor='white', alpha=0.9)
            ax.add_patch(reflection)
        
        # Base eye structure (adjusted for challenges)
        iris = patches.Circle((50, 50), 12, linewidth=2, 
                            edgecolor='gold', facecolor='brown', alpha=max(0.4, iris_conf))
        pupil = patches.Circle((50, 50), 6, linewidth=2, 
                             edgecolor='purple', facecolor='black', alpha=max(0.4, pupil_conf))
        ax.add_patch(iris)
        ax.add_patch(pupil)
        
        # Detection boxes (dashed for uncertainty)
        iris_box = patches.Rectangle((38, 38), 24, 24, linewidth=2, 
                                   edgecolor='gold', facecolor='none', linestyle='--')
        pupil_box = patches.Rectangle((44, 44), 12, 12, linewidth=2, 
                                    edgecolor='purple', facecolor='none', linestyle='--')
        ax.add_patch(iris_box)
        ax.add_patch(pupil_box)
        
        ax.set_title(f'{scenario}\nIris: {iris_conf:.2f} | Pupil: {pupil_conf:.2f}')
        ax.set_xticks([])
        ax.set_yticks([])
        ax.set_aspect('equal')
    
    plt.tight_layout()
    plt.savefig('challenging_scenarios_grid.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Generated: challenging_scenarios_grid.png")

def generate_all_images():
    """Generate all required images"""
    print("üé® Generating analysis images...")
    generate_training_metrics()
    generate_dataset_distribution()
    generate_real_world_detections()
    generate_confidence_examples()
    generate_challenging_scenarios()
    print("‚úÖ All images generated successfully!")

# Load the YOLO model
model = YOLO(r'Eye analysis.pt')

# Path to the video file
video_path = r'eye.mp4'

# Generate analysis images first
generate_all_images()

# Start video capture
cap = cv2.VideoCapture(video_path)

# Check if video file is opened successfully
if not cap.isOpened():
    print("Error: Unable to open video file.")
    exit()

# Desired processing size
target_width = 1280  # Set for HD resolution
target_height = 720

# Output video properties
fps = cap.get(cv2.CAP_PROP_FPS)
output_filename = 'output_video.mp4'

# Define the video writer for .mp4 format with H.264 codec
output_video = cv2.VideoWriter(
    output_filename,
    cv2.VideoWriter_fourcc(*'mp4v'),  # Better codec
    fps,
    (target_width, target_height)
)

print(f"Processing video: {video_path}")
print(f"Output resolution: {target_width}x{target_height}")
print(f"FPS: {fps}")
print("Press 'q' to quit processing")

frame_count = 0

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        print("End of video or unable to read the frame.")
        break

    # Resize the frame to HD (1280x720) resolution
    frame = cv2.resize(frame, (target_width, target_height))

    # Perform detection
    try:
        results = model.predict(frame, verbose=False, conf=0.3)  # Added confidence threshold
        annotated_frame = frame.copy()

        # Initialize detection counters
        iris_count = 0
        pupil_count = 0

        # Add color annotations for iris and pupil
        for r in results:
            if r.boxes is not None:  # Check if detections exist
                for box in r.boxes:
                    cls = int(box.cls[0])
                    confidence = float(box.conf[0])
                    x1, y1, x2, y2 = map(int, box.xyxy[0])
                    
                    if cls == 0:  # Iris
                        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 255), 3)  # Yellow
                        label = f'Iris: {confidence:.2f}'
                        cv2.putText(annotated_frame, label, (x1, y1 - 10), 
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                        iris_count += 1
                        
                    elif cls == 1:  # Pupil
                        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (128, 0, 128), 3)  # Purple
                        label = f'Pupil: {confidence:.2f}'
                        cv2.putText(annotated_frame, label, (x1, y1 - 10), 
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.6, (128, 0, 128), 2)
                        pupil_count += 1
                        
                        # Enhanced segmentation for the pupil
                        if x2 > x1 and y2 > y1:  # Valid coordinates
                            try:
                                pupil_region = frame[y1:y2, x1:x2]
                                if pupil_region.size > 0:  # Check if region is not empty
                                    # Create overlay
                                    overlay = np.zeros_like(pupil_region)
                                    overlay[:, :] = (255, 0, 255)  # Magenta color for segmentation
                                    
                                    # Apply weighted overlay
                                    blended = cv2.addWeighted(pupil_region, 0.6, overlay, 0.4, 0)
                                    annotated_frame[y1:y2, x1:x2] = blended
                            except Exception as e:
                                print(f"Warning: Segmentation error: {e}")

        # Add detection count information to frame
        info_text = f"Iris: {iris_count} | Pupil: {pupil_count} | Frame: {frame_count}"
        cv2.putText(annotated_frame, info_text, (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

    except Exception as e:
        print(f"Error during model prediction: {e}")
        annotated_frame = frame  # Use original frame if prediction fails

    # Display the frame
    cv2.imshow('Eye Analysis', annotated_frame)

    # Write the frame to the output video
    output_video.write(annotated_frame)

    frame_count += 1

    # Progress update every 30 frames
    if frame_count % 30 == 0:
        print(f"Processed {frame_count} frames...")

    # Exit on pressing 'q'
    if cv2.waitKey(1) & 0xFF == ord('q'):
        print("Exiting...")
        break

# Release resources
cap.release()
output_video.release()
cv2.destroyAllWindows()

print(f"Video processing completed!")
print(f"Total frames processed: {frame_count}")
print(f"Output saved as: {output_filename}")
print("\nüìÅ Generated files:")
print("‚Ä¢ training_comprehensive_metrics.png")
print("‚Ä¢ dataset_spatial_distribution.png") 
print("‚Ä¢ real_world_detections_grid.png")
print("‚Ä¢ confidence_score_examples.png")
print("‚Ä¢ challenging_scenarios_grid.png")
print(f"‚Ä¢ {output_filename}")